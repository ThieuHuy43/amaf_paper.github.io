<!-- <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Adaptive Multi-modal Alignment Framework</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

<div class="container">

  <h1>Training-Free Multil-Modal Alignment for Fine-Grained Couterfeit Fruit Detection</h1>
  <p class="affiliation">
  Quynh Nguyen Huu<sup>1*</sup>, Dat Tran-Anh<sup>2*</sup>, Thieu Huy Nguyen<sup>2</sup>, Ngoc Anh Nguyen Thi<sup>1</sup>, Nguyen Huu Gia Bach<sup>3</sup><br>
  <br>
  <sup>1</sup>CMC University, Hanoi, Vietnam<br>
  <sup>2</sup>Faculty of Information Technology, Thuyloi University, Hanoi, Vietnam<br>
  <sup>3</sup>Yen Hoa High School, Hanoi, Vietnam<br>
  <br>
  *Corresponding author
  </p>

  <h3 class="conference">International Conference on Artificial Intelligence: Impacts and Potentials 2025</h3>

  <div class="links">
    <a href="ICAI-IP2025-paper-final.pdf">üìÑ Full Paper</a>
    <a href="ICAI-IP_POSTER_75.pdf">üñºÔ∏è Poster</a>
  </div>

  <h2>Abstract</h2>
  <p class="abstract">
    Fine-grained counterfeit detection remains a challenge due to subtle visual differences, limited samples, and modality inconsistencies between vision and language. We introduce AMAF-Net, an Adaptive Multi-modal Alignment Framework designed for training-free fine-grained recognition. It enhances textual semantics through attribute-guided prompts generated by large language models and refines visual prototypes via base knowledge anchoring. An adaptive fusion mechanism dynamically balances both modalities using confidence-based weighting complemented by lightweight metric refinement. Without retraining, AMAF-Net achieves 90.2% accuracy on the FG-Fruit dataset, surpassing existing vision‚Äìlanguage baselines by up to 5% and demonstrating strong efficiency and robustness for fine-grained counterfeit recognition.
  </p>

  <h2>Method Overview</h2>
  <img src="model.png" alt="method figure" class="figure">

  <h2>Key Results</h2>
  <ul>
    <li>Highest overall accuracy across all benchmark sessions</li>
    <li>Low forgetting and stable performance curve</li>
    <li>Strong fine-grained recognition capability</li>
    <li>Effective for counterfeit fruit detection in real-world settings</li>
  </ul>

  <h2>BibTeX</h2>


</div>

</body>
</html> -->

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AMAF-Net: Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis. com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>

<body>
    <!-- Header with gradient background -->
    <header class="hero-section">
        <div class="hero-content">
            <h1 class="paper-title">Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection</h1>
            <p class="paper-subtitle">AMAF-Net: Adaptive Multi-modal Alignment Framework</p>
            
            <div class="authors">
                <span class="author">Quynh Nguyen Huu<sup>1*</sup></span>
                <span class="author">Dat Tran-Anh<sup>2*</sup></span>
                <span class="author">Thieu Huy Nguyen<sup>2</sup></span>
                <span class="author">Ngoc Anh Nguyen Thi<sup>1</sup></span>
                <span class="author">Nguyen Huu Gia Bach<sup>3</sup></span>
            </div>
            
            <div class="affiliations">
                <div class="affiliation"><sup>1</sup>CMC University, Hanoi, Vietnam</div>
                <div class="affiliation"><sup>2</sup>Faculty of Information Technology, Thuyloi University, Hanoi, Vietnam</div>
                <div class="affiliation"><sup>3</sup>Yen Hoa High School, Hanoi, Vietnam</div>
                <div class="corresponding">*Corresponding authors</div>
            </div>
            
            <div class="conference-badge">
                <i class="fas fa-trophy"></i>
                International Conference on Artificial Intelligence: Impacts and Potentials 2025
            </div>
        </div>
    </header>

    <div class="container">
        <!-- Action buttons -->
        <div class="action-buttons">
            <a href="ICAI-IP2025-paper-final.pdf" class="btn btn-primary" target="_blank">
                <i class="fas fa-file-pdf"></i>
                Read Full Paper
            </a>
            <a href="ICAI-IP_POSTER_75. pdf" class="btn btn-secondary" target="_blank">
                <i class="fas fa-image"></i>
                View Poster
            </a>
            <!-- Uncomment when code is available -->
            <!-- <a href="https://github.com/ThieuHuy43/TF-MCA" class="btn btn-tertiary" target="_blank">
                <i class="fab fa-github"></i>
                Source Code
            </a> -->
        </div>

        <!-- Abstract Section -->
        <section class="content-section">
            <h2><i class="fas fa-lightbulb"></i> Abstract</h2>
            <div class="abstract-box">
                <p>
                    Fine-grained counterfeit detection remains a challenge due to subtle visual differences, limited samples, and modality inconsistencies between vision and language. We introduce <strong>AMAF-Net</strong>, an Adaptive Multi-modal Alignment Framework that addresses these challenges through innovative training-free approaches for precise counterfeit fruit detection.
                </p>
            </div>
        </section>

        <!-- Method Overview -->
        <section class="content-section">
            <h2><i class="fas fa-cogs"></i> Method Overview</h2>
            <div class="method-container">
                <img src="model.png" alt="AMAF-Net Architecture" class="method-figure">
                <p class="method-description">
                    Our AMAF-Net framework leverages adaptive multi-modal alignment to achieve superior performance in fine-grained counterfeit fruit detection without requiring extensive training procedures. 
                </p>
            </div>
        </section>

        <!-- Key Results -->
        <section class="content-section">
            <h2><i class="fas fa-chart-line"></i> Key Results</h2>
            <div class="results-grid">
                <div class="result-item">
                    <div class="result-icon">üéØ</div>
                    <h4>Highest Accuracy</h4>
                    <p>Achieved the highest overall accuracy across all benchmark sessions</p>
                </div>
                <div class="result-item">
                    <div class="result-icon">üìà</div>
                    <h4>Stable Performance</h4>
                    <p>Low forgetting rate and consistent performance curve</p>
                </div>
                <div class="result-item">
                    <div class="result-icon">üîç</div>
                    <h4>Fine-Grained Recognition</h4>
                    <p>Strong capability for detecting subtle differences</p>
                </div>
                <div class="result-item">
                    <div class="result-icon">üåç</div>
                    <h4>Real-World Effective</h4>
                    <p>Proven effectiveness in real-world counterfeit detection scenarios</p>
                </div>
            </div>
        </section>

        <!-- Technical Highlights -->
        <section class="content-section">
            <h2><i class="fas fa-star"></i> Technical Highlights</h2>
            <div class="highlights-container">
                <div class="highlight-item">
                    <h4>Training-Free Approach</h4>
                    <p>No extensive training required, making it highly efficient and deployable</p>
                </div>
                <div class="highlight-item">
                    <h4>Multi-Modal Alignment</h4>
                    <p>Seamlessly integrates vision and language modalities for enhanced detection</p>
                </div>
                <div class="highlight-item">
                    <h4>Adaptive Framework</h4>
                    <p>Dynamically adapts to different counterfeit patterns and scenarios</p>
                </div>
            </div>
        </section>

        <!-- Citation -->
        <section class="content-section citation-section">
            <h2><i class="fas fa-quote-left"></i> Citation</h2>
            <div class="bibtex-container">
                <p class="citation-note">If you use this work in your research, please cite:</p>
                <pre class="bibtex">
@inproceedings{nguyen2025amafnet,
  title={Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection},
  author={Nguyen, Quynh Huu and Tran-Anh, Dat and Nguyen, Thieu Huy and Nguyen, Ngoc Anh and Bach, Nguyen Huu Gia},
  booktitle={International Conference on Artificial Intelligence: Impacts and Potentials},
  year={2025}
}
                </pre>
                <button class="copy-btn" onclick="copyBibtex()">
                    <i class="fas fa-copy"></i> Copy BibTeX
                </button>
            </div>
        </section>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="footer-content">
            <p>&copy; 2025 AMAF-Net Research Team. All rights reserved.</p>
            <div class="footer-links">
                <a href="#" class="footer-link">Contact</a>
                <a href="#" class="footer-link">Acknowledgments</a>
            </div>
        </div>
    </footer>

    <script>
        function copyBibtex() {
            const bibtex = document. querySelector('.bibtex'). textContent;
            navigator.clipboard. writeText(bibtex. trim()). then(() => {
                const btn = document.querySelector('.copy-btn');
                const originalText = btn.innerHTML;
                btn.innerHTML = '<i class="fas fa-check"></i> Copied!';
                btn.style.backgroundColor = '#28a745';
                setTimeout(() => {
                    btn.innerHTML = originalText;
                    btn. style.backgroundColor = '';
                }, 2000);
            });
        }

        // Smooth scrolling for internal links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });

        // Add fade-in animation on scroll
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.classList.add('fade-in');
                }
            });
        }, observerOptions);

        document. querySelectorAll('.content-section'). forEach(section => {
            observer.observe(section);
        });
    </script>
</body>
</html>
